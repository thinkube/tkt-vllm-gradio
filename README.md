# vLLM Inference Server Template for Thinkube

A GPU-accelerated vLLM inference server template for the [Thinkube](https://github.com/thinkube/thinkube) platform, providing high-performance text generation with a Gradio UI.

## ğŸš€ Quick Start

[![Deploy to Thinkube](https://img.shields.io/badge/Deploy%20to-Thinkube-blue?style=for-the-badge&logo=kubernetes)](https://thinkube.github.io/tkt-vllm-gradio/deploy.html)

Click the button above to deploy this template to your Thinkube instance!

## âœ¨ Features

- ğŸš€ **vLLM Engine**: High-performance inference with PagedAttention
- ğŸ¨ **Gradio UI**: Interactive chat interface
- ğŸ–¥ï¸ **GPU Required**: Designed for Ampere+ GPUs (RTX 3090, A100, etc.)
- ğŸ“¦ **Pre-built Base Image**: Fast deployments with pre-installed dependencies

## âš ï¸ Hardware Requirements

**This template requires an Ampere or newer GPU architecture:**
- âœ… NVIDIA RTX 3090 (24GB VRAM)
- âœ… NVIDIA RTX 4090 (24GB VRAM)
- âœ… NVIDIA A100, A40, etc.
- âŒ NVIDIA GTX 1080 (Pascal - NOT supported)
- âŒ NVIDIA RTX 2080 (Turing - NOT supported)

## ğŸ“‹ Supported Models

Models tested with RTX 3090 (24GB):
- `mistralai/Mistral-7B-Instruct-v0.2`
- `meta-llama/Llama-2-7b-chat-hf` (requires HF token)
- `codellama/CodeLlama-7b-Instruct-hf`
- `NousResearch/Hermes-2-Pro-Mistral-7B`

## ğŸ”§ Configuration

When deploying through Thinkube Control:

| Parameter | Description | Example |
|-----------|-------------|---------|
| `model_id` | Hugging Face model ID | `mistralai/Mistral-7B-Instruct-v0.2` |

## ğŸ“ Template Structure

```
tkt-vllm-gradio/
â”œâ”€â”€ manifest.yaml          # Template configuration
â”œâ”€â”€ thinkube.yaml          # Deployment specification
â”œâ”€â”€ Dockerfile.jinja       # Container definition
â”œâ”€â”€ server.py.jinja        # vLLM server with Gradio
â”œâ”€â”€ requirements.txt       # vLLM dependency
â””â”€â”€ docs/
    â””â”€â”€ deploy.html        # Deploy button page
```

## ğŸ› ï¸ Technical Details

- **Engine**: vLLM with PagedAttention
- **Base Image**: Custom AI inference image with CUDA 12.4
- **Runtime**: Python 3.12 on Ubuntu 24.04
- **Port**: 7860 (Gradio default)

## ğŸš€ After Deployment

Your vLLM server will be available at:
- **Application**: `https://your-app.yourdomain.com`
- **Health Check**: `https://your-app.yourdomain.com/health`

## âš¡ Performance

vLLM provides significant performance improvements over standard transformers:
- Up to 24x higher throughput
- Efficient memory usage with PagedAttention
- Support for continuous batching
- Optimized CUDA kernels

## ğŸ“ Development

To customize this template:

1. Fork this repository
2. Modify `server.py.jinja` for custom inference logic
3. Test on a supported GPU (RTX 3090+)
4. Submit a pull request

## âš ï¸ Important Notes

- Models are downloaded on first run (can take time for large models)
- Some models require Hugging Face authentication token
- vLLM only supports specific model architectures (see vLLM docs)
- GPU memory usage depends on model size

## ğŸ¤ Contributing

Contributions welcome! Please test on appropriate hardware (RTX 3090+) before submitting.

## ğŸ“„ License

This template is part of the Thinkube project and follows the same license terms.

---

**Note**: This template requires Ampere+ GPU architecture. It will NOT work on older GPUs like GTX 1080 or RTX 2080.