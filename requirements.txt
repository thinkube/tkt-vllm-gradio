# vLLM - High-performance inference engine
# Requires Ampere+ GPU (RTX 3090, A100, etc.)
vllm>=0.5.0